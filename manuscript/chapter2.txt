# Chapter 2: Spark DataFrames and MLLlib Data Types

## Creating DataFrames


## Creating External Tables


## Getting Information from DataFrames


## Manipulating Data Inside DataFrames


### *withColumn* operations

### String functions

### Mathematical functions

### Datetime functions

### User-defined functions (UDFs)

### Window functions


## Transforming DataFrames


## Joining DataFrames


## Performing Aggregations


## Writing DataFrame Data to Files


## MLlib Data Types


**MLlib** supports both dense and sparse types for vectors and matrices. Vectors are most commonly used in **MLlib** whereas matrices have poor scaling properties.
 
A dense vector contains an array of values, while a sparse vector stores the size of the vector, an array of indices, and an array of values that correspond to the indices. A sparse vector saves space by not storing zero values. For example, with the dense vector `[2.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0]`, a way to store that as a sparse vector is with size `7`, indices as `[0, 3]`, and values as `[2.0, 3.0]`.

A great way to get help when using **Python** is to use the help function on an object or method.  If help isn't sufficient, other places to look include the [programming guides](http://spark.apache.org/docs/latest/programming-guide.html), the [**Python API**](http://spark.apache.org/docs/latest/api/python/index.html), and directly in the [source code](https://github.com/apache/spark/tree/master/python/pyspark) for PySpark.


{lang=python}
    # import data types
    from pyspark.mllib.linalg import DenseVector, SparseVector, SparseMatrix
    from pyspark.mllib.linalg import DenseMatrix, Vectors, Matrices
    
    # Get help on dense vectors
    help(Vectors.dense)
    
    # Get help on sparse vectors
    help(Vectors.sparse)

PySpark provides a [DenseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector) class within the module [pyspark.mllib.linalg](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.linalg). A `DenseVector` is used to store arrays of values for use in PySpark. The `DenseVector` actually stores values in a [**NumPy** array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) and delegates calculations to that object.  You can create a new `DenseVector` using `DenseVector()` and passing in an **NumPy** array or a **Python** list.
 
`DenseVector` implements several functions, such as `DenseVector.dot()` and `DenseVector.norm()`.
 
Note that `DenseVector` stores all values as `np.float64`, so even when passing in a **NumPy** array of integers, the resulting `DenseVector` will contain floating-point numbers. Also, `DenseVector` objects exist locally and are not inherently distributed.  `DenseVector` objects can be used in the distributed setting by including them in `RDDs` or `DataFrames`.
 
A dense vector can be created by using the [Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors) object and calling `Vectors.dense`.  The `Vectors` object also contains a method for creating `SparseVectors`.

{lang=python}
    # Create a DenseVector using Vectors
    denseVector = Vectors.dense([1, 2, 3])
    
    print 'type(denseVector): {0}'.format(type(denseVector))
    print '\ndenseVector: {0}'.format(denseVector)

The dot product of two vectors can be calculated by using `DenseVector.dot()`. The dot product is equivalent to performing element-wise multiplication and then taking the sum of the result.
 
Here is the calculation for the dot product of two vectors, where each vector has length {$$}n{/$$}:

{$$}
\left[ w \cdot x = \sum_{i=1}^n w_i x_i \right]
{/$$} 

