# Chapter 2: Spark DataFrames and MLLlib Data Types

## Creating DataFrames


## Creating External Tables


## Getting Information from DataFrames


## Manipulating Data Inside DataFrames


### *withColumn* operations

### String functions

### Mathematical functions

### Datetime functions

### User-defined functions (UDFs)

### Window functions


## Transforming DataFrames


## Joining DataFrames


## Performing Aggregations


## Writing DataFrame Data to Files


## MLlib Data Types


**MLlib** supports both dense and sparse types for vectors and matrices. Vectors are most commonly used in **MLlib** whereas matrices have poor scaling properties.
 
A dense vector contains an array of values, while a sparse vector stores the size of the vector, an array of indices, and an array of values that correspond to the indices. A sparse vector saves space by not storing zero values. For example, with the dense vector `[2.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0]`, a way to store that as a sparse vector is with size `7`, indices as `[0, 3]`, and values as `[2.0, 3.0]`.

A great way to get help when using **Python** is to use the help function on an object or method.  If help isn't sufficient, other places to look include the [programming guides](http://spark.apache.org/docs/latest/programming-guide.html), the [**Python API**](http://spark.apache.org/docs/latest/api/python/index.html), and directly in the [source code](https://github.com/apache/spark/tree/master/python/pyspark) for PySpark.


{lang=python}
# import data types
from pyspark.mllib.linalg import DenseVector, SparseVector, SparseMatrix, DenseMatrix, Vectors, Matrices

# Get help on dense vectors
help(Vectors.dense)

# Get help on sparse vectors
help(Vectors.sparse)


PySpark provides a [DenseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector) class within the module [pyspark.mllib.linalg](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.linalg). A `DenseVector` is used to store arrays of values for use in PySpark. The `DenseVector` actually stores values in a [**NumPy** array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) and delegates calculations to that object.  You can create a new `DenseVector` using `DenseVector()` and passing in an **NumPy** array or a **Python** list.
 
`DenseVector` implements several functions, such as `DenseVector.dot()` and `DenseVector.norm()`.
 
Note that `DenseVector` stores all values as `np.float64`, so even when passing in a **NumPy** array of integers, the resulting `DenseVector` will contain floating-point numbers. Also, `DenseVector` objects exist locally and are not inherently distributed.  `DenseVector` objects can be used in the distributed setting by including them in `RDDs` or `DataFrames`.
 
A dense vector can be created by using the [Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors) object and calling `Vectors.dense`.  The `Vectors` object also contains a method for creating `SparseVectors`.

{lang=python}
# Create a DenseVector using Vectors
denseVector = Vectors.dense([1, 2, 3])

print 'type(denseVector): {0}'.format(type(denseVector))
print '\ndenseVector: {0}'.format(denseVector)

The dot product of two vectors can be calculated by using `DenseVector.dot()`. The dot product is equivalent to performing element-wise multiplication and then taking the sum of the result.
 
Here is the calculation for the dot product of two vectors, where each vector has length {$$}n{/$$}:

{$$}
\\[ w \cdot x = \sum_{i=1}^n w_i x_i \\]
{/$$} 

Note that you may also see {$$}w \cdot x{/$$} represented as {$$}w^\top x{/$$}.

{lang=python}
denseVector.dot(denseVector)

**Norm**
 
The norm of a vector can be calculated using `Vectors.norm`. The norm calculation is:

{$$} 
\\[ ||x|| _p = \bigg( \sum_i^n |x_i|^p \bigg)^{1/p} \\]
{/$$}

Sometimes it’s important to normalize ML features before training a model. The `spark.ml` library can be used to perform this normalization with a transformer.

{lang=python}
# Get the norm of the `denseVector` object
Vectors.norm(denseVector, 2)

In **Python**, `DenseVector` operations are delegated to an underlying **NumPy** array so that multiplication, addition, division, and other operations can easily be performed.

{lang=python}
# Add 8 to the `denseVector` object
denseVector + 8

# Multiply the `denseVector` object with itself
denseVector * denseVector

Occasionally, an array may be needed from a vector object. Both sparse and dense vectors can be converted to arrays by calling the `toArray` method on the vector.

{lang=python}
denseArray = denseVector.toArray()
print denseArray

**SparseVector**

Create a `SparseVector` using `Vectors.sparse`.

{lang=python}
sparseVector = Vectors.sparse(15, [3, 2], [6.0, 3.0])
print 'type(sparseVector): {0}'.format(type(sparseVector))
print '\nsparseVector: {0}'.format(sparseVector)


{lang=python}
sparseVector.dot(sparseVector)


{lang=python}
sparseVector.norm(2)


**LabeledPoint**

In **MLlib**, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object. Note that the features and label for a `LabeledPoint` are stored in the `features` and `label` attribute of the object.

{lang=python}
from pyspark.mllib.regression import LabeledPoint
# Get help on the `LabeledPoint` object
help(LabeledPoint)


{lang=python}
labeledPoint = LabeledPoint(2015, [7.0, 11.0, 2.0])
print 'labeledPoint: {0}'.format(labeledPoint)

print '\nlabeledPoint.features: {0}'.format(labeledPoint.features)

# Features are being stored as a `DenseVector` type
print 'type(labeledPoint.features): {0}'.format(type(labeledPoint.features))

print '\nlabeledPoint.label: {0}'.format(labeledPoint.label)
print 'type(labeledPoint.label): {0}'.format(type(labeledPoint.label))


{lang=python}
labeledPointSparse = LabeledPoint(2015, Vectors.sparse(15, {0: 7.0, 1:11.0, 2: 2.0}))
print 'labeledPointSparse: {0}'.format(labeledPointSparse)

print '\nlabeledPoint.featuresSparse: {0}'.format(labeledPointSparse.features)
print 'type(labeledPointSparse.features): {0}'.format(type(labeledPointSparse.features))


**Rating**

When performing collaborative filtering, vectors or labeled points aren’t used. Rather, another object is required to capture the relationship between users, products, and ratings. This is represented by the `Rating` object. Further information can be found in the [**Python** API](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.Rating).

{lang=python}
from pyspark.mllib.recommendation import Rating
# Get information on the `Rating` object
help(Rating)

